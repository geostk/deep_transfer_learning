{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from csv import DictReader\n",
    "# Transfer Learning using convolutional neural network (Inception) trained over Imagenet\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os.path\n",
    "import re\n",
    "import sys\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inception's functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'model_dir', '/tmp/imagenet',\n",
    "    \"\"\"Path to classify_image_graph_def.pb, \"\"\"\n",
    "    \"\"\"imagenet_synset_to_human_label_map.txt, and \"\"\"\n",
    "    \"\"\"imagenet_2012_challenge_label_map_proto.pbtxt.\"\"\")\n",
    "tf.app.flags.DEFINE_string('image_file', '',\n",
    "                           \"\"\"Absolute path to image file.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_top_predictions', 5,\n",
    "                            \"\"\"Display this many predictions.\"\"\")\n",
    "\n",
    "# pylint: disable=line-too-long\n",
    "DATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "# pylint: enable=line-too-long\n",
    "\n",
    "class NodeLookup(object):\n",
    "  \"\"\"Converts integer node ID's to human readable labels.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               label_lookup_path=None,\n",
    "               uid_lookup_path=None):\n",
    "    if not label_lookup_path:\n",
    "      label_lookup_path = os.path.join(\n",
    "          FLAGS.model_dir, 'imagenet_2012_challenge_label_map_proto.pbtxt')\n",
    "    if not uid_lookup_path:\n",
    "      uid_lookup_path = os.path.join(\n",
    "          FLAGS.model_dir, 'imagenet_synset_to_human_label_map.txt')\n",
    "    self.node_lookup = self.load(label_lookup_path, uid_lookup_path)\n",
    "\n",
    "  def load(self, label_lookup_path, uid_lookup_path):\n",
    "    \"\"\"Loads a human readable English name for each softmax node.\n",
    "\n",
    "    Args:\n",
    "      label_lookup_path: string UID to integer node ID.\n",
    "      uid_lookup_path: string UID to human-readable string.\n",
    "\n",
    "    Returns:\n",
    "      dict from integer node ID to human-readable string.\n",
    "    \"\"\"\n",
    "    if not tf.gfile.Exists(uid_lookup_path):\n",
    "      tf.logging.fatal('File does not exist %s', uid_lookup_path)\n",
    "    if not tf.gfile.Exists(label_lookup_path):\n",
    "      tf.logging.fatal('File does not exist %s', label_lookup_path)\n",
    "\n",
    "    # Loads mapping from string UID to human-readable string\n",
    "    proto_as_ascii_lines = tf.gfile.GFile(uid_lookup_path).readlines()\n",
    "    uid_to_human = {}\n",
    "    p = re.compile(r'[n\\d]*[ \\S,]*')\n",
    "    for line in proto_as_ascii_lines:\n",
    "      parsed_items = p.findall(line)\n",
    "      uid = parsed_items[0]\n",
    "      human_string = parsed_items[2]\n",
    "      uid_to_human[uid] = human_string\n",
    "\n",
    "    # Loads mapping from string UID to integer node ID.\n",
    "    node_id_to_uid = {}\n",
    "    proto_as_ascii = tf.gfile.GFile(label_lookup_path).readlines()\n",
    "    for line in proto_as_ascii:\n",
    "      if line.startswith('  target_class:'):\n",
    "        target_class = int(line.split(': ')[1])\n",
    "      if line.startswith('  target_class_string:'):\n",
    "        target_class_string = line.split(': ')[1]\n",
    "        node_id_to_uid[target_class] = target_class_string[1:-2]\n",
    "\n",
    "    # Loads the final mapping of integer node ID to human-readable string\n",
    "    node_id_to_name = {}\n",
    "    for key, val in node_id_to_uid.items():\n",
    "      if val not in uid_to_human:\n",
    "        tf.logging.fatal('Failed to locate: %s', val)\n",
    "      name = uid_to_human[val]\n",
    "      node_id_to_name[key] = name\n",
    "\n",
    "    return node_id_to_name\n",
    "\n",
    "  def id_to_string(self, node_id):\n",
    "    if node_id not in self.node_lookup:\n",
    "      return ''\n",
    "    return self.node_lookup[node_id]\n",
    "\n",
    "def create_graph():\n",
    "  \"\"\"Creates a graph from saved GraphDef file and returns a saver.\"\"\"\n",
    "  # Creates graph from saved graph_def.pb.\n",
    "  with tf.gfile.FastGFile(os.path.join(\n",
    "      FLAGS.model_dir, 'classify_image_graph_def.pb'), 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n",
    "    \n",
    "#print all op names\n",
    "def print_ops():\n",
    "    create_graph()\n",
    "    with tf.Session() as sess:\n",
    "        ops = sess.graph.get_operations()\n",
    "        for op in ops:\n",
    "            print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    create_graph()\n",
    "\n",
    "    image_data = tf.gfile.FastGFile('datasets/test/panda.jpg', 'rb').read()\n",
    "\n",
    "    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\n",
    "    predictions = sess.run(softmax_tensor,{'DecodeJpeg/contents:0': image_data})\n",
    "\n",
    "    # Classification\n",
    "    predictions = np.squeeze(predictions)\n",
    "    node_lookup = NodeLookup()\n",
    "    top_k = predictions.argsort()[-FLAGS.num_top_predictions:][::-1]\n",
    "    for node_id in top_k:\n",
    "      human_string = node_lookup.id_to_string(node_id)\n",
    "      score = predictions[node_id]\n",
    "      print('%s (score = %.5f)' % (human_string, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving generated features in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to generate features\n",
    "def generateFeatures(layer_name ,dataset, name):\n",
    "    \"\"\"Generate and save features as csv for a particular layer and dataset.\n",
    "    Keyword arguments:\n",
    "    layer_name -- String: the name of the tensor, ex 'pool_3:0'\n",
    "    dataset -- Generator: an iterator over the image dataset\n",
    "    name -- String: the name of the dataset\n",
    "    \"\"\" \n",
    "    create_graph()\n",
    "    all_features = []\n",
    "    directory = os.path.join('features',name)\n",
    "    filepath = os.path.join(directory,layer_name+\".csv\")\n",
    "    with tf.Session() as sess:\n",
    "        layer = sess.graph.get_tensor_by_name(layer_name)\n",
    "        for image_data in dataset:\n",
    "            features = sess.run(layer,{'DecodeJpeg/contents:0': image_data})\n",
    "            features = np.reshape(features,(np.product(features.shape)))\n",
    "            all_features.append(features)\n",
    "    all_features= np.asarray(all_features)\n",
    "        \n",
    "    # if one wants to see the result without saving\n",
    "    #labels = []\n",
    "    #with open(\"datasets/synthetic/trainLabels.csv\",'rU') as f:\n",
    "    #    rows = csv.DictReader(f)\n",
    "    #    for row in rows:\n",
    "    #        labels.append(row['Class'])\n",
    "    #labels = np.asarray(labels).astype(int)\n",
    "    #from sklearn.manifold import TSNE\n",
    "    #tsne_model = TSNE(n_components=2, random_state=0)\n",
    "    #np.set_printoptions(suppress=True)\n",
    "    #points = tsne_model.fit_transform(all_features)\n",
    "    #plt.scatter(points[:,0],points[:,1], c=labels)\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    np.savetxt(filepath, all_features, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions to create iterator for datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to create image set iterator\n",
    "def dataset_gen(samplesPath, data_dir):\n",
    "    rows = DictReader(open(samplesPath,'rU'))\n",
    "    for row in rows:\n",
    "        filepath = data_dir+'/'+row['Id']+'.jpg'\n",
    "        if not os.DictReaderpath.exists(filepath):\n",
    "            tf.logging.fatal('File does not exist %s', filepath)\n",
    "        yield open(filepath, 'rb').read()\n",
    "        #print(\"Processing: \"+ row['Id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all possible operations (layers) that one can put as layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_ops()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_name = \"pool_3:0\"\n",
    "image_itor = dataset_gen(\"datasets/synthetic/trainLabels.csv\",\"datasets/synthetic\")\n",
    "generateFeatures(layer_name, image_itor, \"synthetic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emotion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_name = \"pool_3:0\"\n",
    "image_itor = dataset_gen(\"datasets/emotion/samples.csv\",\"datasets/emotion\")\n",
    "generateFeatures(layer_name, image_itor, \"emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
